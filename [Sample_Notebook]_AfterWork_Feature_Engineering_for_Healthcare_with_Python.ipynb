{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "a6kTldm5vwo8",
        "IEazGyZjvy_K",
        "WuoRpCYyv2Q3",
        "lmaHm8Zyv4yV",
        "ngDWLvJnzBST",
        "JDYwRtO1zBbW",
        "sPG_3ZqzzBbX",
        "4JViFnHmzD_h",
        "nYwZdOTWzD_i",
        "Q7tO5RL-zD_i",
        "S_mSeXWvzD_j",
        "Grkbkl0azD_j",
        "jWKcg0GUzD_j",
        "i2WCbf5JzEQJ",
        "7kWcPrmYzEQK",
        "_LrNCtEqzEQL",
        "RrOEejy9zEQM",
        "gOHv-dYGzEdh",
        "9hMhxiwezEdj",
        "593ds5QBzEdk",
        "wamLxG9FzEuv",
        "Nnbc4xS6zEux",
        "ppPmzR0hzFRX",
        "CYxnp4e6zFRY",
        "Vz0xt6-xd7_i",
        "3E0IYWCid8Bu",
        "P1Ukkk5ad8B3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-code-tech/100-Days-of-Code-Data-Science/blob/main/%5BSample_Notebook%5D_AfterWork_Feature_Engineering_for_Healthcare_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Sample Notebook] AfterWork: Feature Engineering for Healthcare with Python"
      ],
      "metadata": {
        "id": "U0VbJAy8vu3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisites"
      ],
      "metadata": {
        "id": "a6kTldm5vwo8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OJwszLLavokt"
      },
      "outputs": [],
      "source": [
        "# Import pandas for data manipulation\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import numpy for scientific computations\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sA8mwYGWIhO9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Encoding Features"
      ],
      "metadata": {
        "id": "IEazGyZjvy_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Label Encoding"
      ],
      "metadata": {
        "id": "WuoRpCYyv2Q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label encoding is a method used to convert categorical data into numerical format. This is important because many machine learning algorithms require numerical input data. We can use label encoding when we have categorical features with no inherent order or ranking. For example, if we have a feature 'Color' with categories 'Red', 'Blue', and 'Green', we can assign numerical labels such as 0, 1, and 2 respectively."
      ],
      "metadata": {
        "id": "Ez7Jr_Eg1afe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_942b5.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "pxcHqVNmv4bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create separate LabelEncoder instances for each categorical column\n",
        "gender_encoder = LabelEncoder()\n",
        "blood_type_encoder = LabelEncoder()\n",
        "insurance_type_encoder = LabelEncoder()\n",
        "\n",
        "# Perform label encoding for each column\n",
        "data['Gender'] = gender_encoder.fit_transform(data['Gender'])\n",
        "data['Blood_Type'] = blood_type_encoder.fit_transform(data['Blood_Type'])\n",
        "data['Insurance_Type'] = insurance_type_encoder.fit_transform(data['Insurance_Type'])\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "data.head()"
      ],
      "metadata": {
        "id": "QWTtJJU11jgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "lmaHm8Zyv4yV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply label encoding to the 'Insurance_Type' column in the healthcare dataset from the URL: https://afterwork.ai/ds/ch/healthcare_kzo4n.csv. Remember to use the 'LabelEncoder' class from the 'sklearn.preprocessing' module in Python.\n"
      ],
      "metadata": {
        "id": "hm7-QiLv1qjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/ch/healthcare_kzo4n.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "g5VhWcbKv8fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "DoSCbyoj10cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Ordinal Encoding"
      ],
      "metadata": {
        "id": "ngDWLvJnzBST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordinal encoding is a method of encoding categorical variables where each unique category is assigned a unique integer value based on the order or rank of the category. This encoding is suitable for variables where the categories have a meaningful order or hierarchy.\n",
        "\n"
      ],
      "metadata": {
        "id": "iwYPxwIpzQj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_7eptm.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "6fsV_W19zBSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "ordinal_mapping = {'Former': 0, 'Current': 1, 'Negative': 0, 'Positive': 1}\n",
        "data['Smoking_Status'] = data['Smoking_Status'].map(ordinal_mapping)\n",
        "data['Diabetes_Status'] = data['Diabetes_Status'].map(ordinal_mapping)\n",
        "data['Heart_Disease_Status'] = data['Heart_Disease_Status'].map(ordinal_mapping)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "vf_LK-THzW4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 One-Hot Encoding"
      ],
      "metadata": {
        "id": "JDYwRtO1zBbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot encoding is a technique used in machine learning to convert categorical data into a numerical format. We do this by creating binary columns for each category, where only one column has a value of 1 while the rest have a value of 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "cwCt90423Vst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the provided URL\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_fqkrh.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "SZ1wQxrNzBbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "data_encoded = pd.get_dummies(data, columns=['Gender', 'Blood_Type', 'Diabetes', 'Hypertension', 'Heart_Disease', 'Smoker'])\n",
        "data_encoded.head()"
      ],
      "metadata": {
        "id": "sOONHLqO3p9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "sPG_3ZqzzBbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply one hot encoding to the 'Blood Type' column in the healthcare dataset from the URL: https://afterwork.ai/ds/ch/healthcare_gzwmj.csv. Remember to use pandas get_dummies() function to create binary columns for each category."
      ],
      "metadata": {
        "id": "eGjkajkT5Dt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the provided URL\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/ch/healthcare_gzwmj.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "3II5aX6WzBbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply One Hot Encoding to the 'Blood Type' column\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "-8Jqpf9X5Jvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature Scaling"
      ],
      "metadata": {
        "id": "4JViFnHmzD_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Standardization"
      ],
      "metadata": {
        "id": "nYwZdOTWzD_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization is a technique used to rescale the range of independent variables or features so that they have a mean of 0 and a standard deviation of 1. This process helps in bringing all the features to a similar scale, which is important for many machine learning algorithms that are sensitive to the scale of the input data."
      ],
      "metadata": {
        "id": "UeYQj8U4-PZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_46o8b.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "QOM2kwPGzD_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data[['Age', 'Height', 'Weight', 'Glucose', 'Heart_Rate', 'Exercise_Hours']])\n",
        "\n",
        "data[['Age', 'Height', 'Weight', 'Glucose', 'Heart_Rate', 'Exercise_Hours']] = data_scaled\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Cub_eZ3a-Yxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Min-Max Scaling"
      ],
      "metadata": {
        "id": "Q7tO5RL-zD_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max scaling is a feature scaling technique that rescales the data to a fixed range, usually between 0 and 1. This is done by subtracting the minimum value of the feature and then dividing by the range of the feature (maximum - minimum). This concept is important because it helps to normalize the data and bring all features to a similar scale, which can improve the performance of machine learning algorithms that are sensitive to the scale of the input data."
      ],
      "metadata": {
        "id": "cv8sF0unADtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_s6lr2.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "uO2q8AA4zD_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data[['Age', 'Weight', 'Height', 'Heart_Rate', 'Temperature', 'Cholesterol', 'Glucose', 'Exercise_Hours']])\n",
        "data[['Age', 'Weight', 'Height', 'Heart_Rate', 'Temperature', 'Cholesterol', 'Glucose', 'Exercise_Hours']] = data_scaled\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Z-LmLRjOAND7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform Min-Max scaling on the 'Height' and 'Weight' columns of the healthcare dataset from the URL: https://afterwork.ai/ds/ch/healthcare_xdymn.csv."
      ],
      "metadata": {
        "id": "ENPTwfMwArWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "S_mSeXWvzD_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/ch/healthcare_xdymn.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ELklLsDwzD_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "DCWPxmkKBCiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Robust Scaling"
      ],
      "metadata": {
        "id": "Grkbkl0azD_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Robust Scaling in Feature Scaling involves scaling our features in a way that is not affected by outliers or extreme values. This is important because it helps improve the performance of machine learning models by making them more robust to variations in data.\n",
        "\n"
      ],
      "metadata": {
        "id": "rUh7e_l0BqWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_grjf5.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "tkVZOjxKE5p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Write your code here\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ts7vo-1hzD_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "jWKcg0GUzD_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply Robust Scaling to the 'Height' feature in the healthcare dataset from the URL: https://afterwork.ai/ds/ch/healthcare_bd82k.csv\n"
      ],
      "metadata": {
        "id": "duj-WjUYHh-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://afterwork.ai/ds/ch/healthcare_bd82k.csv\")\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "UzytpahczD_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "robust_scaler = RobustScaler()\n",
        "data['Height'] = robust_scaler.fit_transform(data[['Height']])\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "eSz_2WFZHrWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Date/time feature extraction"
      ],
      "metadata": {
        "id": "i2WCbf5JzEQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Basic Date/Time Features"
      ],
      "metadata": {
        "id": "7kWcPrmYzEQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting basic date/time features involves extracting fundamental information from date/time data such as day of the week, month, year, hour, minute, and second. This allows us to gain insights and patterns from the temporal aspect of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "QmOz6Nx2JTzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_0ikm9.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "iZI-hYv6zEQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Techniques\n",
        "data['DOB'] = pd.to_datetime(data['DOB'])\n",
        "data['Admission_Date'] = pd.to_datetime(data['Admission_Date'])\n",
        "data['Discharge_Date'] = pd.to_datetime(data['Discharge_Date'])\n",
        "\n",
        "data['DOB_day'] = data['DOB'].dt.day\n",
        "data['DOB_month'] = data['DOB'].dt.month\n",
        "data['DOB_year'] = data['DOB'].dt.year\n",
        "\n",
        "data['Admission_dayofweek'] = data['Admission_Date'].dt.dayofweek\n",
        "data['Admission_month'] = data['Admission_Date'].dt.month\n",
        "data['Admission_year'] = data['Admission_Date'].dt.year\n",
        "\n",
        "data['Discharge_hour'] = data['Discharge_Date'].dt.hour\n",
        "data['Discharge_minute'] = data['Discharge_Date'].dt.minute\n",
        "data['Discharge_second'] = data['Discharge_Date'].dt.second\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kdp0VnoeJatC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Calculating Elapsed Time"
      ],
      "metadata": {
        "id": "_LrNCtEqzEQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating elapsed time in healthcare data analysis involves determining the time that has passed between two specific dates or times. This concept is important for tracking patient wait times, medication administration intervals, or the duration of a medical procedure.\n",
        "\n"
      ],
      "metadata": {
        "id": "IdehxJxb4hkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "df = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_5r3du.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YDY8KSB3zEQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Exploration\n",
        "df['Admission_Date'] = pd.to_datetime(df['Admission_Date'])\n",
        "df['Discharge_Date'] = pd.to_datetime(df['Discharge_Date'])\n",
        "df['Medication_Start'] = pd.to_datetime(df['Medication_Start'])\n",
        "df['Medication_End'] = pd.to_datetime(df['Medication_End'])\n",
        "\n",
        "# Feature Engineering Technique\n",
        "df['Medication_Elapsed_Time'] = df['Medication_End'] - df['Medication_Start']\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "R2jWmE_Q42zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "RrOEejy9zEQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the average length of stay (in days) for patients in the healthcare dataset from the URL: https://afterwork.ai/ds/ch/healthcare_rlev5.csv. Remember to calculate the elapsed time by subtracting the Admission Date from the Discharge Date for each patient."
      ],
      "metadata": {
        "id": "V7bA68K3gdd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "df = pd.read_csv(\"https://afterwork.ai/ds/ch/healthcare_rlev5.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wN4-HHgZ6wYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datetime conversions\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Calculate Length of Stay\n",
        "# Write your code here\n",
        "\n",
        "# Calculate Average Length of Stay\n",
        "average_length_of_stay = df['Length of Stay'].mean()\n",
        "\n",
        "print(\"Average Length of Stay (in days) for patients: \", average_length_of_stay)"
      ],
      "metadata": {
        "id": "z7TKxocwzEQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Construction"
      ],
      "metadata": {
        "id": "gOHv-dYGzEdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Feature Construction"
      ],
      "metadata": {
        "id": "9hMhxiwezEdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature construction involves creating new features from existing data to improve the performance of machine learning models. We construct features by combining, transforming, or extracting information from the original data. This process is important because it can help us uncover hidden patterns, reduce noise, and enhance the predictive power of our models.\n",
        "\n"
      ],
      "metadata": {
        "id": "rR2o_fQx9ArQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset from the URL\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_phc4t.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "WXw6jxDA9QH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "data['BMI'] = data['Weight'] / ((data['Height']/100) ** 2)\n",
        "data['Risk_Factor'] = data['Cholesterol_Level'] * data['Heart_Rate']\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "-O4vSSB7zEdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "593ds5QBzEdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new features by calculating the Body Mass Index (BMI) for each patient in the healthcare dataset from the URL: https://afterwork.ai/ds/ch/healthcare_rx4fo.csv. Remember that BMI is calculated as weight (kg) divided by height squared (m^2).\n"
      ],
      "metadata": {
        "id": "LmMqmUlS92r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset from the URL\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/ch/healthcare_rx4fo.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "C0aLShc5zEdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ddZkPiZz96zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Reducing Skewness"
      ],
      "metadata": {
        "id": "wamLxG9FzEuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Log Transformation\n"
      ],
      "metadata": {
        "id": "Nnbc4xS6zEux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log transformation is a technique used to reduce skewness in data by applying the natural logarithm to each data point. This helps to make the data more normally distributed, which can improve the performance of machine learning models that assume normality in the data distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "4bOyx72UIOkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_n0rg6.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "tGFiIEmhzEux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "data['Log_Age'] = data['Age'].apply(lambda x: np.log(x))\n",
        "data['Log_Weight'] = data['Weight'].apply(lambda x: np.log(x))\n",
        "data['Log_Height'] = data['Height'].apply(lambda x: np.log(x))\n",
        "data['Log_Blood_Pressure'] = data['Blood_Pressure'].apply(lambda x: np.log(int(x.split('/')[0])) + np.log(int(x.split('/')[1])) / 2)\n",
        "data['Log_Cholesterol'] = data['Cholesterol'].apply(lambda x: np.log(x))\n",
        "data['Log_Glucose'] = data['Glucose'].apply(lambda x: np.log(x))\n",
        "data['Log_Heart_Rate'] = data['Heart_Rate'].apply(lambda x: np.log(x))\n",
        "data['Log_Body_Temperature'] = data['Body_Temperature'].apply(lambda x: np.log(x))\n",
        "data['Log_Respiratory_Rate'] = data['Respiratory_Rate'].apply(lambda x: np.log(x))"
      ],
      "metadata": {
        "id": "MRJImy3DITZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Distributions\n",
        "data.hist(figsize=(15, 10));"
      ],
      "metadata": {
        "id": "EvLSRtqwIdoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Box-Cox Transformation"
      ],
      "metadata": {
        "id": "SwHMprwHzEuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box-Cox Transformation is a statistical technique that allows us to normalize non-normal data by applying a power transformation. This is important because many statistical methods assume that the data is normally distributed, and the Box-Cox Transformation helps us meet this assumption.\n",
        "\n",
        "To apply the Box-Cox Transformation, we first identify the lambda parameter that maximizes the log-likelihood function. Then, we use this lambda value to transform our data by raising it to the power of lambda. This step helps us achieve a more normal distribution in our data, making it suitable for further analysis."
      ],
      "metadata": {
        "id": "rww2lD5lKLLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "df = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_rels7.txt\")\n",
        "\n",
        "# Data Exploration\n",
        "df.head()"
      ],
      "metadata": {
        "id": "iT9v6EgXKUzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "from scipy import stats\n",
        "\n",
        "# Apply Box-Cox Transformation to 'Weight' column\n",
        "df['Weight_BoxCox'], _ = stats.boxcox(df['Weight'])\n",
        "\n",
        "df[['Weight', 'Weight_BoxCox']]"
      ],
      "metadata": {
        "id": "veFAE-D_zEuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist(column='Weight');"
      ],
      "metadata": {
        "id": "0Fn8xHkgKl0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist(column='Weight_BoxCox');"
      ],
      "metadata": {
        "id": "IS2vYEkRKjQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "atSjNibRzEuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the Box-Cox Transformation to the 'Glucose_Level' data from the dataset located at https://afterwork.ai/ds/ch/healthcare_hdu0o.csv. Remember to identify the lambda parameter that maximizes the log-likelihood function and then transform the data by raising it to the power of lambda.\n"
      ],
      "metadata": {
        "id": "UJEWB97ZOJVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "df = pd.read_csv(\"https://afterwork.ai/ds/ch/healthcare_hdu0o.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qnrtUacVOO_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Box-Cox Transformation to 'Glucose_Level' column\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "IYZuinwSzEuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "8tqhb6jfOXf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "xSrdkpeHObGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Feature Selection"
      ],
      "metadata": {
        "id": "ppPmzR0hzFRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Recursive Feature Elimination"
      ],
      "metadata": {
        "id": "CYxnp4e6zFRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursive Feature Elimination is a feature selection technique that recursively removes attributes and builds a model on those attributes that remain. It is based on the idea that the best features are ranked higher and the worst features are ranked lower. This process helps to identify the most important features in a dataset."
      ],
      "metadata": {
        "id": "yOf0oaO4aBcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_minvr.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "TKRRkn6SzFRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X = data.drop(['Patient_ID', 'Gender', 'Blood_Pressure', 'Diabetes_Status', 'Height', 'Cholesterol_Level'], axis=1)\n",
        "y = data['Diabetes_Status']\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "rfe = RFE(model, n_features_to_select=3)\n",
        "fit = rfe.fit(X, y)\n",
        "\n",
        "print(\"Selected Features:\")\n",
        "print(X.columns[fit.support_])"
      ],
      "metadata": {
        "id": "QK9Dh9PmaIxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Dimensionality Reduction"
      ],
      "metadata": {
        "id": "Vz0xt6-xd7_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Dimensionality Reduction"
      ],
      "metadata": {
        "id": "3E0IYWCid8Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a technique used for dimensionality reduction in which we transform our data into a new coordinate system to reduce the number of features while retaining the most important information. This is important because it helps us deal with the curse of dimensionality, reduce computational complexity, and improve model performance by removing redundant or irrelevant features.\n",
        "\n"
      ],
      "metadata": {
        "id": "vK2fUmTnd8Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset from the URL\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/e/healthcare_y2kpc.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "q1BAXrJVd8Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Technique\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(['ID', 'Blood Pressure'], axis=1)\n",
        "y = data['ID']\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Display the transformed data\n",
        "print(X_pca)"
      ],
      "metadata": {
        "id": "6RgvxUXUd8B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "P1Ukkk5ad8B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform Principal Component Analysis (PCA) on the healthcare dataset from the URL: https://afterwork.ai/ds/ch/healthcare_l31hx.csv to reduce the dimensionality of the data. Remember that PCA is used to transform the data into a new coordinate system to retain the most important information while reducing the number of features."
      ],
      "metadata": {
        "id": "NmP2i-E8d8B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset from the URL\n",
        "data = pd.read_csv(\"https://afterwork.ai/ds/ch/healthcare_l31hx.csv\")\n",
        "\n",
        "# Data Exploration\n",
        "data.head()"
      ],
      "metadata": {
        "id": "XHiC7scmd8B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create separate LabelEncoder instances for each categorical column\n",
        "gender_encoder = LabelEncoder()\n",
        "cholesterol_encoder = LabelEncoder()\n",
        "smoking_encoder = LabelEncoder()\n",
        "\n",
        "# Perform label encoding for each column\n",
        "data['Gender'] = gender_encoder.fit_transform(data['Gender'])\n",
        "data['Cholesterol'] = cholesterol_encoder.fit_transform(data['Cholesterol'])\n",
        "data['Smoking Status'] = smoking_encoder.fit_transform(data['Smoking Status'])\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "data.head()"
      ],
      "metadata": {
        "id": "b6336fUNevJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target variable\n",
        "X = data.drop(['ID', 'Blood Pressure', 'Cholesterol'], axis=1)\n",
        "y = data['ID']\n",
        "\n",
        "# Apply PCA\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Display the transformed data\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "kx95Qgq6d8B_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}