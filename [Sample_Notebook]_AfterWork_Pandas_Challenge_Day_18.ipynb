{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "ghE7giMqYeZs",
        "DqtoC3idYgRF",
        "F-fjVSucZUoA",
        "9y2jPbISZaio",
        "GlGMQNCiZavw",
        "LoelMkicZavx",
        "Cc7em8wSZa3z",
        "lPtdPvHVZbBD",
        "Eo-aBkJ0ZbBD",
        "6DZ9VHxPZbJj",
        "C4mjkJ_GZbTQ",
        "4lDuphVaZbTR",
        "fVg6XCrkZbeO",
        "N7b6M7wEZbpd",
        "KUgmLSBWZbpe",
        "vUhC61gCZb4Y"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-code-tech/100-Days-of-Code-Data-Science/blob/main/%5BSample_Notebook%5D_AfterWork_Pandas_Challenge_Day_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Sample Notebook] AfterWork: Pandas Challenge - Day 18"
      ],
      "metadata": {
        "id": "4QPk9QOtYcN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisite"
      ],
      "metadata": {
        "id": "ghE7giMqYeZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrVp5qhCYTlU"
      },
      "outputs": [],
      "source": [
        "# Import pandas for data manipulation\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Removing Duplicates Based on Specific Columns"
      ],
      "metadata": {
        "id": "DqtoC3idYgRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove duplicates based on specific columns when we want to ensure that our dataset contains only unique combinations of values in those columns. This helps us maintain data integrity and avoid any misleading analysis that could arise from duplicate entries. For example, in a sales dataset, we may have multiple entries for the same customer ID and product ID combination due to data entry errors or system glitches.\n",
        "\n",
        "To apply this concept, we use the `drop_duplicates()` method in Pandas with the subset parameter specifying the columns on which we want to check for duplicates.\n",
        "\n"
      ],
      "metadata": {
        "id": "j6h5h_KrZTFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_dzjxp.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "yjjXLBezZnFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the dataset size\n",
        "df.shape"
      ],
      "metadata": {
        "id": "v4kmD4ndZvwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicates based on specific columns ('Phone Number')\n",
        "df_unique = df.drop_duplicates(subset=['Phone Number'])\n",
        "df_unique.head()"
      ],
      "metadata": {
        "id": "6E3dm4ixZTwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the dataset size\n",
        "df_unique.shape"
      ],
      "metadata": {
        "id": "4f4DOquEZ7sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "F-fjVSucZUoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the dataset of products available at the URL: https://afterwork.ai/ds/ch/products_zpgf4.csv, create a Python script using Pandas to remove duplicates based on the 'Brand' column. This will ensure that each unique brand is represented only once in the dataset. Write the code to achieve this and display the resulting dataset.\n"
      ],
      "metadata": {
        "id": "4V-CxDD0ZZSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/ch/products_zpgf4.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4IjaWesXaMPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset size\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "HkpE6V-haSUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicates based on the 'Brand' column\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "veSC_SghZZuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset size\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "OcS96_jWaWrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Converting columns to lowercase"
      ],
      "metadata": {
        "id": "9y2jPbISZaio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We convert columns to lowercase when we want to standardize the text data in our DataFrame. For example, if we have a column containing product names, converting it to lowercase ensures that 'Apple' and 'apple' are treated as the same value.\n",
        "\n",
        "A real-life use case for converting columns to lowercase is in text processing tasks such as natural language processing or text mining.\n",
        "\n",
        "To apply this concept in Pandas, we can use the str.lower() method along with the apply() function to convert all values in a specific column to lowercase.\n",
        "\n"
      ],
      "metadata": {
        "id": "mKd3n5FVZaiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_9817z.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "O2IyHOZba1C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting columns to lowercase using str.lower() and apply()\n",
        "df = df.apply(lambda x: x.astype(str).str.lower() if x.dtype == 'object' else x)\n",
        "\n",
        "# Displaying the DataFrame after converting columns to lowercase\n",
        "df.head()"
      ],
      "metadata": {
        "id": "T1CCD4qLZair"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Replacing Specific Characters in Multiple Columns"
      ],
      "metadata": {
        "id": "GlGMQNCiZavw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use this concept when we need to clean and standardize our data by replacing certain characters with others. For example, we may want to replace all instances of a special character like '$' with a standard currency symbol like 'USD' across multiple columns.\n",
        "\n",
        "To apply this concept, we first identify the specific character we want to replace, then use the `replace()` method in Pandas to perform the replacement operation on the desired columns."
      ],
      "metadata": {
        "id": "lfPoVvHZZavw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_0dh78.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "aRCb4Mb1c8qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the specific character to be replaced and the replacement character\n",
        "char_to_replace = '-'\n",
        "replacement_char = ' '\n",
        "\n",
        "# Replace the specific character in multiple columns using the replace() method\n",
        "columns_to_replace = ['First Name', 'Last Name', 'Address', 'Phone Number']\n",
        "df[columns_to_replace] = df[columns_to_replace].replace(char_to_replace, replacement_char, regex=True)\n",
        "\n",
        "# Preview the updated DataFrame after replacing the specific character\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9nGMFMqKZavx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "LoelMkicZavx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the dataset of products available at the following URL: https://afterwork.ai/ds/ch/products_of4qc.csv, create a Python script using Pandas to replace all instances of the color 'Black' in the 'Color' column with 'Midnight Black'. This will help standardize the color naming convention for the products."
      ],
      "metadata": {
        "id": "S_R3gi7uZavy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/ch/products_of4qc.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4cUPkdHUezb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the specific character to be replaced and the replacement character\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Replace the specific character in the 'Color' column using the replace() method\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Display the updated DataFrame after replacing the specific character\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "3KW1hxfHZavy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Sampling Random Rows from a DataFrame for Reproducibility"
      ],
      "metadata": {
        "id": "Cc7em8wSZa3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We sample random rows from a DataFrame to ensure reproducibility in our data analysis. This is important when we want to perform statistical analysis or machine learning tasks on our dataset. For example, in a survey dataset, we can randomly sample rows to analyze the responses without skewing the results.\n",
        "\n",
        "To apply this concept, we use the 'sample' method in Pandas with the 'random_state' parameter set to a specific value. This ensures that every time we run the sampling operation, we get the same set of random rows, making our analysis reproducible.\n",
        "\n"
      ],
      "metadata": {
        "id": "LpzwsjJrZa3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_lmifa.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "lIDvR1aUi7_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the random_state parameter to ensure reproducibility\n",
        "sampled_df = df.sample(n=5, random_state=42)\n",
        "\n",
        "# Displaying the sampled DataFrame\n",
        "sampled_df"
      ],
      "metadata": {
        "id": "qqAYIDbQZa30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Adding Timedelta to Date Column"
      ],
      "metadata": {
        "id": "lPtdPvHVZbBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add timedelta to a date column when we want to perform date arithmetic operations such as adding or subtracting a specific duration from dates in our dataset. This allows us to calculate future or past dates based on a given reference date. For example, we can add 7 days to a date column to calculate the date one week ahead.\n",
        "\n",
        "To apply this concept, we first create a timedelta object representing the duration we want to add or subtract, then use the Pandas library to add this timedelta to the date column in our DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "S_pS7gpmZbBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_m4so6.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Up3XN9FSkmhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date of Birth' column to datetime format\n",
        "df['Date of Birth'] = pd.to_datetime(df['Date of Birth'])\n",
        "\n",
        "# Define a timedelta object representing 30 days\n",
        "from datetime import timedelta\n",
        "delta = timedelta(days=30)\n",
        "\n",
        "# Add the timedelta to the 'Date of Birth' column\n",
        "df['New Date'] = df['Date of Birth'] + delta\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df[['Customer ID', 'First Name', 'Last Name', 'Date of Birth', 'New Date']]"
      ],
      "metadata": {
        "id": "OKXaof4zZbBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "Eo-aBkJ0ZbBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the dataset of products available at the following URL: https://afterwork.ai/ds/ch/products_nwdl9.csv, create a Python script that adds a timedelta of 30 days to the 'Expiry Date' of each product and stores the result in a new column named 'New Expiry Date'. This new column should contain the updated dates after adding the timedelta. Make sure to use the Pandas library for this task.\n"
      ],
      "metadata": {
        "id": "9tf5KaGzZbBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/ch/products_nwdl9.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Fes4rAdZmNAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Expiry Date' column to datetime format\n",
        "# Write your code here\n",
        "\n",
        "# Define the timedelta to add (e.g., 30 days)\n",
        "# Write your code here\n",
        "\n",
        "# Add the timedelta to the 'Expiry Date' column and store in 'New Expiry Date'\n",
        "# Write your code here\n",
        "\n",
        "# Preview the updated DataFrame\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "2x2ohQ4NZbBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Subtracting Timedelta from Date Column"
      ],
      "metadata": {
        "id": "6DZ9VHxPZbJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We subtract a timedelta from a date column to calculate a new date by subtracting a specific duration from the original date. This is useful when we need to calculate deadlines, project durations, or track events that occurred before or after a certain period. For example, we can subtract 7 days from a date column to find the date a week before a specific event.\n",
        "\n",
        "To apply this concept, we first create a timedelta object with the desired duration (e.g., 7 days) and then subtract this timedelta from the date column using the Pandas library in Python. This operation will result in a new date column with the adjusted dates.\n",
        "\n"
      ],
      "metadata": {
        "id": "sphjJG1CZbJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_gsw08.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2abRyhhamg8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date of Birth' column to datetime format\n",
        "df['Date of Birth'] = pd.to_datetime(df['Date of Birth'])\n",
        "\n",
        "# Define the timedelta to subtract (e.g., 30 days)\n",
        "delta = timedelta(days=30)\n",
        "\n",
        "# Subtract the defined timedelta from the 'Date of Birth' column\n",
        "df['New Date'] = df['Date of Birth'] - delta\n",
        "\n",
        "# Display the updated DataFrame with the new date column\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KVRUQUjMZbJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Shifting Dates Using DateOffset"
      ],
      "metadata": {
        "id": "C4mjkJ_GZbTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the DateOffset class in Pandas to shift dates by a specified amount. This allows us to easily manipulate dates in our DataFrame by adding or subtracting days, months, years, etc.\n",
        "\n",
        "We can use this concept to perform time-based calculations, such as comparing data from different time periods or creating time series forecasts. For example, we can shift the dates of sales data to compare monthly sales figures year-over-year.\n",
        "\n",
        "To apply this concept, we first create a DateOffset object with the desired shift value (e.g., DateOffset(days=7) to shift dates by 7 days) and then use the .apply() method along with the DateOffset object to shift the dates in a specific column of our DataFrame."
      ],
      "metadata": {
        "id": "e_znsLF3ZbTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_iqyku.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4lOTLciMnTZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Join Date' column to datetime format\n",
        "df['Join Date'] = pd.to_datetime(df['Join Date'])\n",
        "\n",
        "# Define a DateOffset object to shift dates by 1 year\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "offset = DateOffset(years=1)\n",
        "\n",
        "# Apply the DateOffset to shift the 'Join Date' column by 1 year\n",
        "df['Join Date Shifted'] = df['Join Date'] + offset\n",
        "\n",
        "# Display the original 'Join Date' and the shifted 'Join Date' columns\n",
        "df[['Join Date', 'Join Date Shifted']]"
      ],
      "metadata": {
        "id": "-mLxhJKNZbTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "4lDuphVaZbTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the dataset of products available at the URL: https://afterwork.ai/ds/ch/products_7xlud.csv, create a Python script using Pandas that shifts the 'Expiry Date' of each product by 6 months ahead. This will help in analyzing the inventory management strategy for the upcoming months. Remember to use the DateOffset class in Pandas for date shifting.\n"
      ],
      "metadata": {
        "id": "qu0DnAhrZbTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/ch/products_7xlud.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GcEHRCKznxbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Expiry Date' column to datetime format\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Define a DateOffset object to shift dates by 6 months\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Apply the DateOffset to shift the 'Expiry Date' column by 6 months\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Display the original 'Expiry Date' and the shifted 'Expiry Date' columns\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "Ix8SJZKUZbTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Deleting columns using the pop method"
      ],
      "metadata": {
        "id": "fVg6XCrkZbeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We delete columns using the pop method when we want to remove a specific column from a DataFrame in Pandas. For example, if we have a dataset with a column that contains sensitive information that we do not need for our analysis, we can use the pop method to delete that column.\n",
        "\n",
        "To apply this concept, we simply call the pop method on the DataFrame and pass the column label as an argument, like this: df.pop('column_name'). This will remove the specified column from the DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "lqHLnUCzZbeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_9hs6k.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "UtWMTIYXoRLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting the 'Phone Number' column using the pop method\n",
        "df.pop('Phone Number')\n",
        "\n",
        "# Previewing the DataFrame after deleting the column\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1I2E8w_AZbeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Creating Crosstab Tables with Aggregation"
      ],
      "metadata": {
        "id": "N7b6M7wEZbpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create crosstab tables with aggregation to summarize and analyze the relationship between two or more categorical variables. For example, we can create a crosstab table with aggregation to compare the sales performance of different products across various regions.\n",
        "\n",
        "To apply this concept, we use the pandas library in Python to call the crosstab function and specify the variables we want to analyze along with the aggregation function we want to apply, such as sum, mean, count, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "QPi81k2UZbpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_af8rn.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "H2SvgAKto3R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a crosstab table to analyze the relationship between Gender and State\n",
        "gender_state_crosstab = pd.crosstab(df['Gender'], df['State'])\n",
        "\n",
        "# Displaying the crosstab table\n",
        "gender_state_crosstab"
      ],
      "metadata": {
        "id": "KMgNZ78PZbpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"green\">Challenge</font>"
      ],
      "metadata": {
        "id": "KUgmLSBWZbpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the provided sales dataset from the URL: https://afterwork.ai/ds/ch/sales_73y8p.csv, create a crosstab table to analyze the relationship between 'Product Category' and 'Payment Method'. This will help you understand how different product categories are purchased using various payment methods. Use the pd.crosstab() function in Pandas to achieve this analysis.\n"
      ],
      "metadata": {
        "id": "buTsESE0Zbpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/ch/sales_73y8p.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uoKu0d5Zp_Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a crosstab table to analyze the relationship between 'Product Category' and 'Payment Method'\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Displaying the crosstab table\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "59Tl-cFfZbpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Filtering with loc and isin based on Multiple Conditions"
      ],
      "metadata": {
        "id": "vUhC61gCZb4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We filter data in a DataFrame using the loc function combined with the isin method based on multiple conditions. For example, we can filter rows where the values in one column are within a certain range and another column matches specific values.\n",
        "\n",
        "A real-life use case for this concept would be in analyzing sales data where we want to extract transactions that occurred within a certain date range and involved specific products.\n",
        "\n",
        "To apply this concept, we first use the loc function to select rows based on one condition and then further filter those rows using the isin method to check for values in another column, ensuring that both conditions are met.\n",
        "\n"
      ],
      "metadata": {
        "id": "EB3G7mALZb4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset from the provided URL\n",
        "df = pd.read_csv('https://afterwork.ai/ds/e/customers_rxkhp.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cxct1rvZqXvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering the data using loc and isin based on multiple conditions\n",
        "filtered_data = df.loc[(df['Age'] >= 30) & (df['State'].isin(['CA', 'TX']))]\n",
        "\n",
        "# Displaying the filtered data\n",
        "filtered_data"
      ],
      "metadata": {
        "id": "PVSeLrnHZb4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}